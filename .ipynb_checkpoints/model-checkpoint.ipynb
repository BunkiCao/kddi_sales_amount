{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import Series, DataFrame\n",
    "from sklearn.decomposition import PCA \n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import RMSprop\n",
    "from keras import backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the data\n",
    "\n",
    "dataset is [datax, datay]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "datax = pd.read_csv('../feature.csv', sep='\\t')\n",
    "datay = pd.read_csv('output_y.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>deal_category_cd</th>\n",
       "      <th>shop_id</th>\n",
       "      <th>deal_type_cd</th>\n",
       "      <th>deal_price</th>\n",
       "      <th>discount_rate</th>\n",
       "      <th>month</th>\n",
       "      <th>duration_days</th>\n",
       "      <th>dspt_vec0</th>\n",
       "      <th>dspt_vec1</th>\n",
       "      <th>dspt_vec2</th>\n",
       "      <th>...</th>\n",
       "      <th>dspt_vec190</th>\n",
       "      <th>dspt_vec191</th>\n",
       "      <th>dspt_vec192</th>\n",
       "      <th>dspt_vec193</th>\n",
       "      <th>dspt_vec194</th>\n",
       "      <th>dspt_vec195</th>\n",
       "      <th>dspt_vec196</th>\n",
       "      <th>dspt_vec197</th>\n",
       "      <th>dspt_vec198</th>\n",
       "      <th>dspt_vec199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>13869</td>\n",
       "      <td>1</td>\n",
       "      <td>3200</td>\n",
       "      <td>64</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.487003</td>\n",
       "      <td>-0.487012</td>\n",
       "      <td>-0.300625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.129280</td>\n",
       "      <td>-0.008112</td>\n",
       "      <td>0.047803</td>\n",
       "      <td>-0.412086</td>\n",
       "      <td>-0.186055</td>\n",
       "      <td>-0.428090</td>\n",
       "      <td>-1.028613</td>\n",
       "      <td>-1.133246</td>\n",
       "      <td>0.102500</td>\n",
       "      <td>0.565922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.047797</td>\n",
       "      <td>-0.363245</td>\n",
       "      <td>0.328044</td>\n",
       "      <td>...</td>\n",
       "      <td>0.739456</td>\n",
       "      <td>-0.497679</td>\n",
       "      <td>0.344068</td>\n",
       "      <td>-0.614602</td>\n",
       "      <td>-0.931943</td>\n",
       "      <td>-0.404058</td>\n",
       "      <td>0.269973</td>\n",
       "      <td>-0.481750</td>\n",
       "      <td>-0.354643</td>\n",
       "      <td>0.680272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>10479</td>\n",
       "      <td>2</td>\n",
       "      <td>3240</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.236814</td>\n",
       "      <td>-0.042894</td>\n",
       "      <td>-0.312521</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.496862</td>\n",
       "      <td>0.399478</td>\n",
       "      <td>-0.418426</td>\n",
       "      <td>-0.057347</td>\n",
       "      <td>-0.541013</td>\n",
       "      <td>-0.008830</td>\n",
       "      <td>-0.787262</td>\n",
       "      <td>-0.314165</td>\n",
       "      <td>-0.278627</td>\n",
       "      <td>0.008451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.035394</td>\n",
       "      <td>-0.398830</td>\n",
       "      <td>0.502443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.606086</td>\n",
       "      <td>-0.485434</td>\n",
       "      <td>0.371555</td>\n",
       "      <td>-0.571355</td>\n",
       "      <td>-0.752632</td>\n",
       "      <td>-0.153454</td>\n",
       "      <td>0.511548</td>\n",
       "      <td>-0.381676</td>\n",
       "      <td>-0.207083</td>\n",
       "      <td>0.606146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9</td>\n",
       "      <td>8334</td>\n",
       "      <td>2</td>\n",
       "      <td>1400</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.045568</td>\n",
       "      <td>-0.002191</td>\n",
       "      <td>-0.356119</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.610163</td>\n",
       "      <td>0.271743</td>\n",
       "      <td>0.026410</td>\n",
       "      <td>-0.070720</td>\n",
       "      <td>-0.734749</td>\n",
       "      <td>-0.021287</td>\n",
       "      <td>-0.570350</td>\n",
       "      <td>-0.251306</td>\n",
       "      <td>-0.455745</td>\n",
       "      <td>0.053695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.157187</td>\n",
       "      <td>-0.196884</td>\n",
       "      <td>0.430383</td>\n",
       "      <td>...</td>\n",
       "      <td>0.556017</td>\n",
       "      <td>-0.516333</td>\n",
       "      <td>0.292942</td>\n",
       "      <td>-0.510219</td>\n",
       "      <td>-0.815570</td>\n",
       "      <td>-0.341179</td>\n",
       "      <td>0.393145</td>\n",
       "      <td>-0.299472</td>\n",
       "      <td>-0.290749</td>\n",
       "      <td>0.524980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>9</td>\n",
       "      <td>8334</td>\n",
       "      <td>2</td>\n",
       "      <td>5480</td>\n",
       "      <td>37</td>\n",
       "      <td>12</td>\n",
       "      <td>14</td>\n",
       "      <td>-0.287976</td>\n",
       "      <td>0.077439</td>\n",
       "      <td>-0.724518</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.730756</td>\n",
       "      <td>0.463583</td>\n",
       "      <td>-0.163860</td>\n",
       "      <td>-0.099533</td>\n",
       "      <td>-0.726087</td>\n",
       "      <td>-0.057904</td>\n",
       "      <td>-1.049588</td>\n",
       "      <td>-0.672365</td>\n",
       "      <td>-0.428750</td>\n",
       "      <td>0.020867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>11</td>\n",
       "      <td>6593</td>\n",
       "      <td>2</td>\n",
       "      <td>6800</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.401356</td>\n",
       "      <td>0.231198</td>\n",
       "      <td>-0.780717</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.495988</td>\n",
       "      <td>0.503905</td>\n",
       "      <td>0.189778</td>\n",
       "      <td>-0.091715</td>\n",
       "      <td>-0.374547</td>\n",
       "      <td>0.154913</td>\n",
       "      <td>-0.720336</td>\n",
       "      <td>-1.186944</td>\n",
       "      <td>-0.503544</td>\n",
       "      <td>0.383948</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.188005</td>\n",
       "      <td>-0.162056</td>\n",
       "      <td>0.440517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.485737</td>\n",
       "      <td>-0.509055</td>\n",
       "      <td>0.205669</td>\n",
       "      <td>-0.514193</td>\n",
       "      <td>-0.662836</td>\n",
       "      <td>-0.327128</td>\n",
       "      <td>0.408174</td>\n",
       "      <td>-0.290132</td>\n",
       "      <td>-0.232235</td>\n",
       "      <td>0.469798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>9833</td>\n",
       "      <td>2</td>\n",
       "      <td>3500</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.282669</td>\n",
       "      <td>-0.026581</td>\n",
       "      <td>-0.610132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.822827</td>\n",
       "      <td>0.854255</td>\n",
       "      <td>-0.094156</td>\n",
       "      <td>0.024080</td>\n",
       "      <td>-0.746256</td>\n",
       "      <td>0.198009</td>\n",
       "      <td>-1.250182</td>\n",
       "      <td>-0.429074</td>\n",
       "      <td>-0.149078</td>\n",
       "      <td>0.051133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.128200</td>\n",
       "      <td>0.045878</td>\n",
       "      <td>0.281513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.308597</td>\n",
       "      <td>-0.465505</td>\n",
       "      <td>0.119238</td>\n",
       "      <td>-0.345226</td>\n",
       "      <td>-0.369610</td>\n",
       "      <td>-0.179150</td>\n",
       "      <td>0.217760</td>\n",
       "      <td>-0.489515</td>\n",
       "      <td>-0.208441</td>\n",
       "      <td>0.365243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>7003</td>\n",
       "      <td>2</td>\n",
       "      <td>12744</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.123809</td>\n",
       "      <td>-0.500418</td>\n",
       "      <td>-0.557116</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.798181</td>\n",
       "      <td>0.496638</td>\n",
       "      <td>-0.034171</td>\n",
       "      <td>0.047562</td>\n",
       "      <td>-0.595119</td>\n",
       "      <td>-0.197406</td>\n",
       "      <td>-0.501120</td>\n",
       "      <td>-0.153037</td>\n",
       "      <td>-0.033588</td>\n",
       "      <td>0.041597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.115541</td>\n",
       "      <td>0.058118</td>\n",
       "      <td>0.279813</td>\n",
       "      <td>...</td>\n",
       "      <td>0.271219</td>\n",
       "      <td>-0.461024</td>\n",
       "      <td>0.110460</td>\n",
       "      <td>-0.341470</td>\n",
       "      <td>-0.363755</td>\n",
       "      <td>-0.203539</td>\n",
       "      <td>0.222835</td>\n",
       "      <td>-0.473956</td>\n",
       "      <td>-0.185541</td>\n",
       "      <td>0.357674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>11</td>\n",
       "      <td>6193</td>\n",
       "      <td>2</td>\n",
       "      <td>9280</td>\n",
       "      <td>49</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.386478</td>\n",
       "      <td>-0.177073</td>\n",
       "      <td>-0.792531</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.423086</td>\n",
       "      <td>0.101370</td>\n",
       "      <td>-0.082357</td>\n",
       "      <td>-0.432843</td>\n",
       "      <td>-0.241997</td>\n",
       "      <td>0.144353</td>\n",
       "      <td>-0.765318</td>\n",
       "      <td>-0.809146</td>\n",
       "      <td>-0.301981</td>\n",
       "      <td>0.406393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>9</td>\n",
       "      <td>695</td>\n",
       "      <td>2</td>\n",
       "      <td>2720</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.234143</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>-1.373209</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.599570</td>\n",
       "      <td>0.246430</td>\n",
       "      <td>0.119353</td>\n",
       "      <td>-0.344474</td>\n",
       "      <td>-0.688489</td>\n",
       "      <td>-0.652439</td>\n",
       "      <td>-1.233067</td>\n",
       "      <td>-0.685956</td>\n",
       "      <td>-0.291937</td>\n",
       "      <td>0.518295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>11</td>\n",
       "      <td>6593</td>\n",
       "      <td>2</td>\n",
       "      <td>5680</td>\n",
       "      <td>70</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.396228</td>\n",
       "      <td>0.213973</td>\n",
       "      <td>-0.792319</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.404851</td>\n",
       "      <td>0.483681</td>\n",
       "      <td>0.266943</td>\n",
       "      <td>-0.074845</td>\n",
       "      <td>-0.191163</td>\n",
       "      <td>-0.001009</td>\n",
       "      <td>-0.788630</td>\n",
       "      <td>-1.102237</td>\n",
       "      <td>-0.454041</td>\n",
       "      <td>0.441027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>8</td>\n",
       "      <td>8181</td>\n",
       "      <td>2</td>\n",
       "      <td>58800</td>\n",
       "      <td>35</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.170619</td>\n",
       "      <td>-0.007355</td>\n",
       "      <td>-0.798893</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444160</td>\n",
       "      <td>-0.942218</td>\n",
       "      <td>0.674338</td>\n",
       "      <td>0.095165</td>\n",
       "      <td>-0.134390</td>\n",
       "      <td>0.134493</td>\n",
       "      <td>-0.558301</td>\n",
       "      <td>-0.147834</td>\n",
       "      <td>-0.064407</td>\n",
       "      <td>1.276860</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>9</td>\n",
       "      <td>7803</td>\n",
       "      <td>2</td>\n",
       "      <td>3380</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.531523</td>\n",
       "      <td>-0.309804</td>\n",
       "      <td>-0.638614</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.645092</td>\n",
       "      <td>0.205699</td>\n",
       "      <td>-0.168021</td>\n",
       "      <td>0.015694</td>\n",
       "      <td>0.081968</td>\n",
       "      <td>-0.835978</td>\n",
       "      <td>-0.747488</td>\n",
       "      <td>-0.562503</td>\n",
       "      <td>-0.397835</td>\n",
       "      <td>0.741557</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>11</td>\n",
       "      <td>10588</td>\n",
       "      <td>2</td>\n",
       "      <td>10800</td>\n",
       "      <td>50</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.351277</td>\n",
       "      <td>-0.078291</td>\n",
       "      <td>-0.230442</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.311994</td>\n",
       "      <td>-0.045970</td>\n",
       "      <td>-0.014367</td>\n",
       "      <td>-0.066862</td>\n",
       "      <td>-0.617492</td>\n",
       "      <td>-0.068873</td>\n",
       "      <td>-1.022171</td>\n",
       "      <td>-0.477465</td>\n",
       "      <td>-0.520893</td>\n",
       "      <td>0.471650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.926550</td>\n",
       "      <td>0.067344</td>\n",
       "      <td>-0.257554</td>\n",
       "      <td>...</td>\n",
       "      <td>0.587577</td>\n",
       "      <td>-1.270145</td>\n",
       "      <td>0.994396</td>\n",
       "      <td>-0.302188</td>\n",
       "      <td>-0.737445</td>\n",
       "      <td>-0.133326</td>\n",
       "      <td>-0.666286</td>\n",
       "      <td>-1.021507</td>\n",
       "      <td>-0.539557</td>\n",
       "      <td>1.246123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>9</td>\n",
       "      <td>7803</td>\n",
       "      <td>2</td>\n",
       "      <td>2380</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.237342</td>\n",
       "      <td>0.178606</td>\n",
       "      <td>-0.376371</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.609663</td>\n",
       "      <td>0.778754</td>\n",
       "      <td>-0.055481</td>\n",
       "      <td>-0.107367</td>\n",
       "      <td>0.102709</td>\n",
       "      <td>-0.443452</td>\n",
       "      <td>-1.046089</td>\n",
       "      <td>-0.405869</td>\n",
       "      <td>-0.566007</td>\n",
       "      <td>0.708938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.007312</td>\n",
       "      <td>-0.127235</td>\n",
       "      <td>0.257701</td>\n",
       "      <td>...</td>\n",
       "      <td>0.561627</td>\n",
       "      <td>-0.564382</td>\n",
       "      <td>0.305069</td>\n",
       "      <td>-0.577740</td>\n",
       "      <td>-0.671729</td>\n",
       "      <td>-0.384483</td>\n",
       "      <td>0.165135</td>\n",
       "      <td>-0.730714</td>\n",
       "      <td>-0.392566</td>\n",
       "      <td>0.554844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>8</td>\n",
       "      <td>1346</td>\n",
       "      <td>2</td>\n",
       "      <td>1580</td>\n",
       "      <td>55</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.735003</td>\n",
       "      <td>-0.136742</td>\n",
       "      <td>-0.293428</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077213</td>\n",
       "      <td>0.152694</td>\n",
       "      <td>0.163258</td>\n",
       "      <td>-0.273771</td>\n",
       "      <td>-0.668004</td>\n",
       "      <td>-0.534815</td>\n",
       "      <td>-1.101571</td>\n",
       "      <td>-0.882956</td>\n",
       "      <td>-0.201606</td>\n",
       "      <td>0.517318</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.506973</td>\n",
       "      <td>-0.386190</td>\n",
       "      <td>0.149488</td>\n",
       "      <td>...</td>\n",
       "      <td>0.309152</td>\n",
       "      <td>-0.954982</td>\n",
       "      <td>0.490891</td>\n",
       "      <td>-0.646224</td>\n",
       "      <td>-0.320509</td>\n",
       "      <td>-0.003347</td>\n",
       "      <td>-0.147305</td>\n",
       "      <td>-0.241840</td>\n",
       "      <td>-0.089176</td>\n",
       "      <td>1.076155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.257347</td>\n",
       "      <td>-0.276665</td>\n",
       "      <td>0.235596</td>\n",
       "      <td>...</td>\n",
       "      <td>0.534776</td>\n",
       "      <td>-0.852070</td>\n",
       "      <td>0.461537</td>\n",
       "      <td>-0.706874</td>\n",
       "      <td>-0.598863</td>\n",
       "      <td>-0.253726</td>\n",
       "      <td>0.041629</td>\n",
       "      <td>-0.600450</td>\n",
       "      <td>-0.295463</td>\n",
       "      <td>0.916539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>11</td>\n",
       "      <td>6593</td>\n",
       "      <td>2</td>\n",
       "      <td>9280</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>11</td>\n",
       "      <td>-0.776009</td>\n",
       "      <td>-0.432403</td>\n",
       "      <td>-0.545910</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.245174</td>\n",
       "      <td>-0.237321</td>\n",
       "      <td>0.481851</td>\n",
       "      <td>-0.230093</td>\n",
       "      <td>-0.150229</td>\n",
       "      <td>0.017457</td>\n",
       "      <td>-1.193761</td>\n",
       "      <td>-0.993947</td>\n",
       "      <td>0.029922</td>\n",
       "      <td>0.078122</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>8</td>\n",
       "      <td>1346</td>\n",
       "      <td>2</td>\n",
       "      <td>1780</td>\n",
       "      <td>40</td>\n",
       "      <td>12</td>\n",
       "      <td>9</td>\n",
       "      <td>-0.348670</td>\n",
       "      <td>-0.214932</td>\n",
       "      <td>-0.604360</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.516591</td>\n",
       "      <td>0.204983</td>\n",
       "      <td>0.135566</td>\n",
       "      <td>-0.233460</td>\n",
       "      <td>-0.303186</td>\n",
       "      <td>-0.477247</td>\n",
       "      <td>-1.597516</td>\n",
       "      <td>-0.973911</td>\n",
       "      <td>-0.582838</td>\n",
       "      <td>0.461862</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>8</td>\n",
       "      <td>10029</td>\n",
       "      <td>2</td>\n",
       "      <td>3130</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.391155</td>\n",
       "      <td>-0.303647</td>\n",
       "      <td>0.123172</td>\n",
       "      <td>...</td>\n",
       "      <td>0.267519</td>\n",
       "      <td>-0.735663</td>\n",
       "      <td>0.378948</td>\n",
       "      <td>-0.485111</td>\n",
       "      <td>-0.255728</td>\n",
       "      <td>0.017503</td>\n",
       "      <td>-0.121854</td>\n",
       "      <td>-0.206911</td>\n",
       "      <td>-0.082722</td>\n",
       "      <td>0.824879</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>8</td>\n",
       "      <td>7057</td>\n",
       "      <td>2</td>\n",
       "      <td>42800</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.204241</td>\n",
       "      <td>-0.025110</td>\n",
       "      <td>-0.784132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.735772</td>\n",
       "      <td>0.705962</td>\n",
       "      <td>-0.017798</td>\n",
       "      <td>-0.401693</td>\n",
       "      <td>-0.786946</td>\n",
       "      <td>-0.567913</td>\n",
       "      <td>-0.768496</td>\n",
       "      <td>-0.712166</td>\n",
       "      <td>-0.304170</td>\n",
       "      <td>0.336035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>8</td>\n",
       "      <td>8181</td>\n",
       "      <td>2</td>\n",
       "      <td>56800</td>\n",
       "      <td>34</td>\n",
       "      <td>12</td>\n",
       "      <td>8</td>\n",
       "      <td>-1.090399</td>\n",
       "      <td>-0.464217</td>\n",
       "      <td>0.276184</td>\n",
       "      <td>...</td>\n",
       "      <td>0.742424</td>\n",
       "      <td>-1.542101</td>\n",
       "      <td>1.046133</td>\n",
       "      <td>-0.560571</td>\n",
       "      <td>-0.606171</td>\n",
       "      <td>-0.999523</td>\n",
       "      <td>-0.809106</td>\n",
       "      <td>-0.690177</td>\n",
       "      <td>-0.823230</td>\n",
       "      <td>1.042582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161660</th>\n",
       "      <td>8</td>\n",
       "      <td>14568</td>\n",
       "      <td>2</td>\n",
       "      <td>3880</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.244394</td>\n",
       "      <td>-0.036187</td>\n",
       "      <td>0.262550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.245717</td>\n",
       "      <td>-0.488430</td>\n",
       "      <td>0.257835</td>\n",
       "      <td>-0.369078</td>\n",
       "      <td>-0.261161</td>\n",
       "      <td>-0.058940</td>\n",
       "      <td>0.227857</td>\n",
       "      <td>-0.409396</td>\n",
       "      <td>-0.266207</td>\n",
       "      <td>0.457578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161661</th>\n",
       "      <td>4</td>\n",
       "      <td>5760</td>\n",
       "      <td>2</td>\n",
       "      <td>1280</td>\n",
       "      <td>90</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.311009</td>\n",
       "      <td>-0.054989</td>\n",
       "      <td>-0.523517</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.630911</td>\n",
       "      <td>0.349283</td>\n",
       "      <td>-0.416309</td>\n",
       "      <td>-0.458495</td>\n",
       "      <td>-0.735173</td>\n",
       "      <td>-0.514960</td>\n",
       "      <td>-1.641106</td>\n",
       "      <td>-0.524691</td>\n",
       "      <td>-0.580222</td>\n",
       "      <td>0.445142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161662</th>\n",
       "      <td>8</td>\n",
       "      <td>1602</td>\n",
       "      <td>2</td>\n",
       "      <td>10890</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.220137</td>\n",
       "      <td>-0.317815</td>\n",
       "      <td>0.069469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.262536</td>\n",
       "      <td>-0.122276</td>\n",
       "      <td>0.296788</td>\n",
       "      <td>-0.127964</td>\n",
       "      <td>-0.231869</td>\n",
       "      <td>0.141711</td>\n",
       "      <td>-0.028734</td>\n",
       "      <td>0.171878</td>\n",
       "      <td>0.042835</td>\n",
       "      <td>0.175718</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161663</th>\n",
       "      <td>8</td>\n",
       "      <td>14568</td>\n",
       "      <td>2</td>\n",
       "      <td>3880</td>\n",
       "      <td>40</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.243134</td>\n",
       "      <td>-0.042424</td>\n",
       "      <td>0.259578</td>\n",
       "      <td>...</td>\n",
       "      <td>0.242595</td>\n",
       "      <td>-0.464358</td>\n",
       "      <td>0.249209</td>\n",
       "      <td>-0.368921</td>\n",
       "      <td>-0.290746</td>\n",
       "      <td>-0.071416</td>\n",
       "      <td>0.231255</td>\n",
       "      <td>-0.378495</td>\n",
       "      <td>-0.262877</td>\n",
       "      <td>0.472049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161664</th>\n",
       "      <td>11</td>\n",
       "      <td>4146</td>\n",
       "      <td>2</td>\n",
       "      <td>1980</td>\n",
       "      <td>54</td>\n",
       "      <td>11</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.376320</td>\n",
       "      <td>-0.225015</td>\n",
       "      <td>-0.545370</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.200464</td>\n",
       "      <td>0.231193</td>\n",
       "      <td>-0.065830</td>\n",
       "      <td>-0.444438</td>\n",
       "      <td>-0.510846</td>\n",
       "      <td>-0.613886</td>\n",
       "      <td>-1.462107</td>\n",
       "      <td>-0.592815</td>\n",
       "      <td>-0.477340</td>\n",
       "      <td>0.518003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161665</th>\n",
       "      <td>8</td>\n",
       "      <td>14568</td>\n",
       "      <td>2</td>\n",
       "      <td>5940</td>\n",
       "      <td>38</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>1.002926</td>\n",
       "      <td>-0.290539</td>\n",
       "      <td>0.380767</td>\n",
       "      <td>...</td>\n",
       "      <td>0.881570</td>\n",
       "      <td>1.528145</td>\n",
       "      <td>0.740622</td>\n",
       "      <td>0.169426</td>\n",
       "      <td>-0.248190</td>\n",
       "      <td>0.590384</td>\n",
       "      <td>-0.488691</td>\n",
       "      <td>-2.488954</td>\n",
       "      <td>-0.674918</td>\n",
       "      <td>0.082799</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161666</th>\n",
       "      <td>8</td>\n",
       "      <td>5510</td>\n",
       "      <td>2</td>\n",
       "      <td>10260</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.629800</td>\n",
       "      <td>-0.752491</td>\n",
       "      <td>0.668316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988337</td>\n",
       "      <td>-1.646624</td>\n",
       "      <td>0.716531</td>\n",
       "      <td>-0.773149</td>\n",
       "      <td>-0.871359</td>\n",
       "      <td>-0.846821</td>\n",
       "      <td>-0.220222</td>\n",
       "      <td>-0.860455</td>\n",
       "      <td>-0.591583</td>\n",
       "      <td>0.792477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161667</th>\n",
       "      <td>8</td>\n",
       "      <td>1602</td>\n",
       "      <td>2</td>\n",
       "      <td>12890</td>\n",
       "      <td>48</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.115418</td>\n",
       "      <td>-0.070932</td>\n",
       "      <td>0.118722</td>\n",
       "      <td>...</td>\n",
       "      <td>0.037983</td>\n",
       "      <td>-0.094994</td>\n",
       "      <td>-0.074199</td>\n",
       "      <td>-0.034875</td>\n",
       "      <td>-0.127200</td>\n",
       "      <td>-0.021638</td>\n",
       "      <td>0.035487</td>\n",
       "      <td>-0.097039</td>\n",
       "      <td>-0.065903</td>\n",
       "      <td>0.035869</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161668</th>\n",
       "      <td>11</td>\n",
       "      <td>6047</td>\n",
       "      <td>2</td>\n",
       "      <td>8980</td>\n",
       "      <td>35</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.478515</td>\n",
       "      <td>-0.183207</td>\n",
       "      <td>-1.085258</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.965259</td>\n",
       "      <td>0.248870</td>\n",
       "      <td>0.276991</td>\n",
       "      <td>-0.526010</td>\n",
       "      <td>-0.511087</td>\n",
       "      <td>-0.310662</td>\n",
       "      <td>-0.724324</td>\n",
       "      <td>-0.916873</td>\n",
       "      <td>-0.095296</td>\n",
       "      <td>0.523926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161669</th>\n",
       "      <td>8</td>\n",
       "      <td>1602</td>\n",
       "      <td>2</td>\n",
       "      <td>9890</td>\n",
       "      <td>39</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.910452</td>\n",
       "      <td>-1.030603</td>\n",
       "      <td>0.732469</td>\n",
       "      <td>...</td>\n",
       "      <td>0.974496</td>\n",
       "      <td>-1.967171</td>\n",
       "      <td>0.844175</td>\n",
       "      <td>-0.771562</td>\n",
       "      <td>-0.724822</td>\n",
       "      <td>-1.002738</td>\n",
       "      <td>-0.539640</td>\n",
       "      <td>-0.781583</td>\n",
       "      <td>-0.713322</td>\n",
       "      <td>0.717065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161670</th>\n",
       "      <td>8</td>\n",
       "      <td>14638</td>\n",
       "      <td>2</td>\n",
       "      <td>9080</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.129956</td>\n",
       "      <td>0.057169</td>\n",
       "      <td>-0.220401</td>\n",
       "      <td>...</td>\n",
       "      <td>0.815954</td>\n",
       "      <td>-0.484932</td>\n",
       "      <td>1.336279</td>\n",
       "      <td>-0.209755</td>\n",
       "      <td>-0.230842</td>\n",
       "      <td>0.408734</td>\n",
       "      <td>-0.599108</td>\n",
       "      <td>-0.021398</td>\n",
       "      <td>-0.296130</td>\n",
       "      <td>0.104231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161671</th>\n",
       "      <td>9</td>\n",
       "      <td>16632</td>\n",
       "      <td>2</td>\n",
       "      <td>2480</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.041197</td>\n",
       "      <td>-2.234569</td>\n",
       "      <td>-0.367926</td>\n",
       "      <td>...</td>\n",
       "      <td>0.895123</td>\n",
       "      <td>0.092014</td>\n",
       "      <td>0.208370</td>\n",
       "      <td>-0.920188</td>\n",
       "      <td>-1.113618</td>\n",
       "      <td>0.247282</td>\n",
       "      <td>-2.222413</td>\n",
       "      <td>-1.531642</td>\n",
       "      <td>-0.584029</td>\n",
       "      <td>0.225495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161672</th>\n",
       "      <td>11</td>\n",
       "      <td>6047</td>\n",
       "      <td>2</td>\n",
       "      <td>3980</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.493844</td>\n",
       "      <td>0.013078</td>\n",
       "      <td>-0.801746</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.742665</td>\n",
       "      <td>0.987467</td>\n",
       "      <td>0.279977</td>\n",
       "      <td>-0.102133</td>\n",
       "      <td>-0.027314</td>\n",
       "      <td>-0.069436</td>\n",
       "      <td>-1.131024</td>\n",
       "      <td>-0.983293</td>\n",
       "      <td>0.004859</td>\n",
       "      <td>0.125363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161673</th>\n",
       "      <td>8</td>\n",
       "      <td>5510</td>\n",
       "      <td>2</td>\n",
       "      <td>10260</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.259875</td>\n",
       "      <td>-0.191501</td>\n",
       "      <td>0.415714</td>\n",
       "      <td>...</td>\n",
       "      <td>0.811300</td>\n",
       "      <td>-0.796053</td>\n",
       "      <td>0.139067</td>\n",
       "      <td>-0.842458</td>\n",
       "      <td>-1.116935</td>\n",
       "      <td>-0.385801</td>\n",
       "      <td>0.329509</td>\n",
       "      <td>-0.386164</td>\n",
       "      <td>-0.186081</td>\n",
       "      <td>0.670041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161674</th>\n",
       "      <td>4</td>\n",
       "      <td>4140</td>\n",
       "      <td>2</td>\n",
       "      <td>1350</td>\n",
       "      <td>34</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.198274</td>\n",
       "      <td>0.047610</td>\n",
       "      <td>-0.297980</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.478956</td>\n",
       "      <td>0.645151</td>\n",
       "      <td>-0.157809</td>\n",
       "      <td>-0.121785</td>\n",
       "      <td>-0.716132</td>\n",
       "      <td>-0.209281</td>\n",
       "      <td>-0.886830</td>\n",
       "      <td>-0.303203</td>\n",
       "      <td>-0.681900</td>\n",
       "      <td>0.354667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161675</th>\n",
       "      <td>8</td>\n",
       "      <td>1602</td>\n",
       "      <td>2</td>\n",
       "      <td>17890</td>\n",
       "      <td>41</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.010415</td>\n",
       "      <td>-0.174877</td>\n",
       "      <td>0.065620</td>\n",
       "      <td>...</td>\n",
       "      <td>0.223869</td>\n",
       "      <td>-0.064047</td>\n",
       "      <td>0.033957</td>\n",
       "      <td>0.011888</td>\n",
       "      <td>-0.116730</td>\n",
       "      <td>0.073402</td>\n",
       "      <td>-0.075450</td>\n",
       "      <td>-0.149072</td>\n",
       "      <td>-0.114950</td>\n",
       "      <td>0.115294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161676</th>\n",
       "      <td>4</td>\n",
       "      <td>707</td>\n",
       "      <td>2</td>\n",
       "      <td>5920</td>\n",
       "      <td>28</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.264301</td>\n",
       "      <td>0.231039</td>\n",
       "      <td>-0.527905</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.612378</td>\n",
       "      <td>0.692076</td>\n",
       "      <td>-0.394390</td>\n",
       "      <td>-0.339037</td>\n",
       "      <td>-0.955438</td>\n",
       "      <td>-0.345813</td>\n",
       "      <td>-0.985633</td>\n",
       "      <td>-0.324630</td>\n",
       "      <td>-0.408364</td>\n",
       "      <td>0.048898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161677</th>\n",
       "      <td>8</td>\n",
       "      <td>14638</td>\n",
       "      <td>2</td>\n",
       "      <td>9080</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.160331</td>\n",
       "      <td>0.091304</td>\n",
       "      <td>-0.224035</td>\n",
       "      <td>...</td>\n",
       "      <td>0.820377</td>\n",
       "      <td>-0.489251</td>\n",
       "      <td>1.338132</td>\n",
       "      <td>-0.204903</td>\n",
       "      <td>-0.230891</td>\n",
       "      <td>0.449161</td>\n",
       "      <td>-0.616336</td>\n",
       "      <td>-0.045401</td>\n",
       "      <td>-0.302185</td>\n",
       "      <td>0.098645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161678</th>\n",
       "      <td>8</td>\n",
       "      <td>14638</td>\n",
       "      <td>2</td>\n",
       "      <td>9080</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.320760</td>\n",
       "      <td>0.115008</td>\n",
       "      <td>-0.301157</td>\n",
       "      <td>...</td>\n",
       "      <td>0.917409</td>\n",
       "      <td>-0.551781</td>\n",
       "      <td>1.566371</td>\n",
       "      <td>-0.240333</td>\n",
       "      <td>-0.225475</td>\n",
       "      <td>0.496135</td>\n",
       "      <td>-0.700243</td>\n",
       "      <td>-0.022046</td>\n",
       "      <td>-0.339213</td>\n",
       "      <td>0.111040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161679</th>\n",
       "      <td>9</td>\n",
       "      <td>16632</td>\n",
       "      <td>2</td>\n",
       "      <td>2480</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>-1.269221</td>\n",
       "      <td>-1.708969</td>\n",
       "      <td>-0.034739</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034666</td>\n",
       "      <td>0.216420</td>\n",
       "      <td>0.926310</td>\n",
       "      <td>-1.394728</td>\n",
       "      <td>-1.086774</td>\n",
       "      <td>0.400839</td>\n",
       "      <td>-2.184104</td>\n",
       "      <td>-1.918598</td>\n",
       "      <td>-0.335619</td>\n",
       "      <td>-0.272932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161680</th>\n",
       "      <td>8</td>\n",
       "      <td>4090</td>\n",
       "      <td>2</td>\n",
       "      <td>2990</td>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.214722</td>\n",
       "      <td>-0.159227</td>\n",
       "      <td>-0.565837</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.305005</td>\n",
       "      <td>0.632856</td>\n",
       "      <td>-0.028393</td>\n",
       "      <td>0.042264</td>\n",
       "      <td>-0.731946</td>\n",
       "      <td>-0.441820</td>\n",
       "      <td>-1.198673</td>\n",
       "      <td>-0.652204</td>\n",
       "      <td>-0.352233</td>\n",
       "      <td>0.440388</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161681</th>\n",
       "      <td>2</td>\n",
       "      <td>9280</td>\n",
       "      <td>2</td>\n",
       "      <td>2950</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>12</td>\n",
       "      <td>-0.837002</td>\n",
       "      <td>-0.061286</td>\n",
       "      <td>0.886550</td>\n",
       "      <td>...</td>\n",
       "      <td>0.265967</td>\n",
       "      <td>-0.268992</td>\n",
       "      <td>0.176062</td>\n",
       "      <td>0.111552</td>\n",
       "      <td>-0.737579</td>\n",
       "      <td>-0.292620</td>\n",
       "      <td>-0.686985</td>\n",
       "      <td>-0.291699</td>\n",
       "      <td>-0.417068</td>\n",
       "      <td>-0.156539</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161682</th>\n",
       "      <td>8</td>\n",
       "      <td>1602</td>\n",
       "      <td>2</td>\n",
       "      <td>9980</td>\n",
       "      <td>58</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161683</th>\n",
       "      <td>4</td>\n",
       "      <td>4140</td>\n",
       "      <td>2</td>\n",
       "      <td>1480</td>\n",
       "      <td>31</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>-0.029377</td>\n",
       "      <td>-0.206212</td>\n",
       "      <td>-0.474434</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.641541</td>\n",
       "      <td>0.319282</td>\n",
       "      <td>-0.137391</td>\n",
       "      <td>-0.172887</td>\n",
       "      <td>-0.350624</td>\n",
       "      <td>-0.314643</td>\n",
       "      <td>-0.848174</td>\n",
       "      <td>-0.307982</td>\n",
       "      <td>-0.135648</td>\n",
       "      <td>0.268081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161684</th>\n",
       "      <td>8</td>\n",
       "      <td>14638</td>\n",
       "      <td>2</td>\n",
       "      <td>9080</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.171293</td>\n",
       "      <td>0.069289</td>\n",
       "      <td>-0.211444</td>\n",
       "      <td>...</td>\n",
       "      <td>0.832171</td>\n",
       "      <td>-0.471936</td>\n",
       "      <td>1.327255</td>\n",
       "      <td>-0.222753</td>\n",
       "      <td>-0.244398</td>\n",
       "      <td>0.431547</td>\n",
       "      <td>-0.582547</td>\n",
       "      <td>-0.031168</td>\n",
       "      <td>-0.319898</td>\n",
       "      <td>0.086200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161685</th>\n",
       "      <td>8</td>\n",
       "      <td>1602</td>\n",
       "      <td>2</td>\n",
       "      <td>13890</td>\n",
       "      <td>42</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.060733</td>\n",
       "      <td>-0.082497</td>\n",
       "      <td>0.073288</td>\n",
       "      <td>...</td>\n",
       "      <td>0.191598</td>\n",
       "      <td>-0.081064</td>\n",
       "      <td>-0.083326</td>\n",
       "      <td>0.226049</td>\n",
       "      <td>-0.118010</td>\n",
       "      <td>0.012604</td>\n",
       "      <td>-0.094667</td>\n",
       "      <td>-0.093882</td>\n",
       "      <td>0.008796</td>\n",
       "      <td>0.150246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161686</th>\n",
       "      <td>9</td>\n",
       "      <td>16632</td>\n",
       "      <td>2</td>\n",
       "      <td>2480</td>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>-0.315753</td>\n",
       "      <td>-0.927346</td>\n",
       "      <td>-0.587583</td>\n",
       "      <td>...</td>\n",
       "      <td>0.707110</td>\n",
       "      <td>0.223423</td>\n",
       "      <td>0.458621</td>\n",
       "      <td>-0.042081</td>\n",
       "      <td>0.490015</td>\n",
       "      <td>-0.030354</td>\n",
       "      <td>-2.341384</td>\n",
       "      <td>-0.851811</td>\n",
       "      <td>-0.119273</td>\n",
       "      <td>0.431337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161687</th>\n",
       "      <td>8</td>\n",
       "      <td>5510</td>\n",
       "      <td>2</td>\n",
       "      <td>10260</td>\n",
       "      <td>50</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>-0.629800</td>\n",
       "      <td>-0.752491</td>\n",
       "      <td>0.668316</td>\n",
       "      <td>...</td>\n",
       "      <td>0.988337</td>\n",
       "      <td>-1.646624</td>\n",
       "      <td>0.716531</td>\n",
       "      <td>-0.773149</td>\n",
       "      <td>-0.871359</td>\n",
       "      <td>-0.846821</td>\n",
       "      <td>-0.220222</td>\n",
       "      <td>-0.860455</td>\n",
       "      <td>-0.591583</td>\n",
       "      <td>0.792477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161688</th>\n",
       "      <td>8</td>\n",
       "      <td>14638</td>\n",
       "      <td>2</td>\n",
       "      <td>9890</td>\n",
       "      <td>49</td>\n",
       "      <td>10</td>\n",
       "      <td>9</td>\n",
       "      <td>-1.221835</td>\n",
       "      <td>0.266565</td>\n",
       "      <td>-0.687513</td>\n",
       "      <td>...</td>\n",
       "      <td>0.957084</td>\n",
       "      <td>-0.388220</td>\n",
       "      <td>1.394963</td>\n",
       "      <td>-0.235708</td>\n",
       "      <td>-0.378381</td>\n",
       "      <td>0.281120</td>\n",
       "      <td>-0.751800</td>\n",
       "      <td>-0.300846</td>\n",
       "      <td>-0.570374</td>\n",
       "      <td>0.675513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161689</th>\n",
       "      <td>4</td>\n",
       "      <td>707</td>\n",
       "      <td>2</td>\n",
       "      <td>9980</td>\n",
       "      <td>39</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>-0.156052</td>\n",
       "      <td>0.061990</td>\n",
       "      <td>-1.107024</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.647476</td>\n",
       "      <td>0.530825</td>\n",
       "      <td>-0.297612</td>\n",
       "      <td>-0.350183</td>\n",
       "      <td>-0.950713</td>\n",
       "      <td>-0.442733</td>\n",
       "      <td>-0.985704</td>\n",
       "      <td>-0.550679</td>\n",
       "      <td>-0.845962</td>\n",
       "      <td>0.449780</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>161690 rows × 207 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        deal_category_cd  shop_id  deal_type_cd  deal_price discount_rate  \\\n",
       "0                      3    13869             1        3200            64   \n",
       "1                      8    10029             2        3130             0   \n",
       "2                      2    10479             2        3240             0   \n",
       "3                      8    10029             2        3130             0   \n",
       "4                      9     8334             2        1400            35   \n",
       "5                      8    10029             2        3130             0   \n",
       "6                      9     8334             2        5480            37   \n",
       "7                     11     6593             2        6800            70   \n",
       "8                      8    10029             2        3130             0   \n",
       "9                      9     9833             2        3500             0   \n",
       "10                     8    10029             2        3130             0   \n",
       "11                    11     7003             2       12744             0   \n",
       "12                     8    10029             2        3130             0   \n",
       "13                    11     6193             2        9280            49   \n",
       "14                     9      695             2        2720            50   \n",
       "15                    11     6593             2        5680            70   \n",
       "16                     8     8181             2       58800            35   \n",
       "17                     9     7803             2        3380             0   \n",
       "18                    11    10588             2       10800            50   \n",
       "19                     8    10029             2        3130             0   \n",
       "20                     9     7803             2        2380             0   \n",
       "21                     8    10029             2        3130             0   \n",
       "22                     8     1346             2        1580            55   \n",
       "23                     8    10029             2        3130             0   \n",
       "24                     8    10029             2        3130             0   \n",
       "25                    11     6593             2        9280             0   \n",
       "26                     8     1346             2        1780            40   \n",
       "27                     8    10029             2        3130             0   \n",
       "28                     8     7057             2       42800             0   \n",
       "29                     8     8181             2       56800            34   \n",
       "...                  ...      ...           ...         ...           ...   \n",
       "161660                 8    14568             2        3880            40   \n",
       "161661                 4     5760             2        1280            90   \n",
       "161662                 8     1602             2       10890            58   \n",
       "161663                 8    14568             2        3880            40   \n",
       "161664                11     4146             2        1980            54   \n",
       "161665                 8    14568             2        5940            38   \n",
       "161666                 8     5510             2       10260            50   \n",
       "161667                 8     1602             2       12890            48   \n",
       "161668                11     6047             2        8980            35   \n",
       "161669                 8     1602             2        9890            39   \n",
       "161670                 8    14638             2        9080            50   \n",
       "161671                 9    16632             2        2480             0   \n",
       "161672                11     6047             2        3980            34   \n",
       "161673                 8     5510             2       10260            50   \n",
       "161674                 4     4140             2        1350            34   \n",
       "161675                 8     1602             2       17890            41   \n",
       "161676                 4      707             2        5920            28   \n",
       "161677                 8    14638             2        9080            50   \n",
       "161678                 8    14638             2        9080            50   \n",
       "161679                 9    16632             2        2480             0   \n",
       "161680                 8     4090             2        2990            56   \n",
       "161681                 2     9280             2        2950             0   \n",
       "161682                 8     1602             2        9980            58   \n",
       "161683                 4     4140             2        1480            31   \n",
       "161684                 8    14638             2        9080            50   \n",
       "161685                 8     1602             2       13890            42   \n",
       "161686                 9    16632             2        2480             0   \n",
       "161687                 8     5510             2       10260            50   \n",
       "161688                 8    14638             2        9890            49   \n",
       "161689                 4      707             2        9980            39   \n",
       "\n",
       "        month  duration_days  dspt_vec0  dspt_vec1  dspt_vec2     ...       \\\n",
       "0          12             14  -0.487003  -0.487012  -0.300625     ...        \n",
       "1          12              7  -0.047797  -0.363245   0.328044     ...        \n",
       "2          12              7  -0.236814  -0.042894  -0.312521     ...        \n",
       "3          12              7  -0.035394  -0.398830   0.502443     ...        \n",
       "4          12             14  -0.045568  -0.002191  -0.356119     ...        \n",
       "5          12              7  -0.157187  -0.196884   0.430383     ...        \n",
       "6          12             14  -0.287976   0.077439  -0.724518     ...        \n",
       "7          12             12  -0.401356   0.231198  -0.780717     ...        \n",
       "8          12              7  -0.188005  -0.162056   0.440517     ...        \n",
       "9          12             12  -0.282669  -0.026581  -0.610132     ...        \n",
       "10         12              7  -0.128200   0.045878   0.281513     ...        \n",
       "11         12             12  -0.123809  -0.500418  -0.557116     ...        \n",
       "12         12              7  -0.115541   0.058118   0.279813     ...        \n",
       "13         12              7  -0.386478  -0.177073  -0.792531     ...        \n",
       "14         12             12  -0.234143   0.092630  -1.373209     ...        \n",
       "15         12             12  -0.396228   0.213973  -0.792319     ...        \n",
       "16         12              8  -1.170619  -0.007355  -0.798893     ...        \n",
       "17         12             10  -0.531523  -0.309804  -0.638614     ...        \n",
       "18         12             10  -0.351277  -0.078291  -0.230442     ...        \n",
       "19         12              7  -0.926550   0.067344  -0.257554     ...        \n",
       "20         12             10  -0.237342   0.178606  -0.376371     ...        \n",
       "21         12              7  -0.007312  -0.127235   0.257701     ...        \n",
       "22         12              9  -0.735003  -0.136742  -0.293428     ...        \n",
       "23         12              7  -0.506973  -0.386190   0.149488     ...        \n",
       "24         12              7  -0.257347  -0.276665   0.235596     ...        \n",
       "25         12             11  -0.776009  -0.432403  -0.545910     ...        \n",
       "26         12              9  -0.348670  -0.214932  -0.604360     ...        \n",
       "27         12              7  -0.391155  -0.303647   0.123172     ...        \n",
       "28         12              7  -0.204241  -0.025110  -0.784132     ...        \n",
       "29         12              8  -1.090399  -0.464217   0.276184     ...        \n",
       "...       ...            ...        ...        ...        ...     ...        \n",
       "161660     10              7  -0.244394  -0.036187   0.262550     ...        \n",
       "161661     10             10  -0.311009  -0.054989  -0.523517     ...        \n",
       "161662     10              7  -0.220137  -0.317815   0.069469     ...        \n",
       "161663     10              7  -0.243134  -0.042424   0.259578     ...        \n",
       "161664     11             10  -0.376320  -0.225015  -0.545370     ...        \n",
       "161665     10              7   1.002926  -0.290539   0.380767     ...        \n",
       "161666     10              7  -0.629800  -0.752491   0.668316     ...        \n",
       "161667     10              7  -0.115418  -0.070932   0.118722     ...        \n",
       "161668     10             10  -0.478515  -0.183207  -1.085258     ...        \n",
       "161669     10              7  -0.910452  -1.030603   0.732469     ...        \n",
       "161670     10              9  -1.129956   0.057169  -0.220401     ...        \n",
       "161671     11              3  -1.041197  -2.234569  -0.367926     ...        \n",
       "161672     10             10  -0.493844   0.013078  -0.801746     ...        \n",
       "161673     10              7  -0.259875  -0.191501   0.415714     ...        \n",
       "161674     10             10  -0.198274   0.047610  -0.297980     ...        \n",
       "161675     10              7  -0.010415  -0.174877   0.065620     ...        \n",
       "161676      9              6  -0.264301   0.231039  -0.527905     ...        \n",
       "161677     10              9  -1.160331   0.091304  -0.224035     ...        \n",
       "161678     10              9  -1.320760   0.115008  -0.301157     ...        \n",
       "161679     11              3  -1.269221  -1.708969  -0.034739     ...        \n",
       "161680     10              7  -0.214722  -0.159227  -0.565837     ...        \n",
       "161681      9             12  -0.837002  -0.061286   0.886550     ...        \n",
       "161682     10              7        NaN        NaN        NaN     ...        \n",
       "161683     10             10  -0.029377  -0.206212  -0.474434     ...        \n",
       "161684     10              9  -1.171293   0.069289  -0.211444     ...        \n",
       "161685     10              7  -0.060733  -0.082497   0.073288     ...        \n",
       "161686     11              3  -0.315753  -0.927346  -0.587583     ...        \n",
       "161687     10              7  -0.629800  -0.752491   0.668316     ...        \n",
       "161688     10              9  -1.221835   0.266565  -0.687513     ...        \n",
       "161689      9              6  -0.156052   0.061990  -1.107024     ...        \n",
       "\n",
       "        dspt_vec190  dspt_vec191  dspt_vec192  dspt_vec193  dspt_vec194  \\\n",
       "0         -0.129280    -0.008112     0.047803    -0.412086    -0.186055   \n",
       "1          0.739456    -0.497679     0.344068    -0.614602    -0.931943   \n",
       "2         -0.496862     0.399478    -0.418426    -0.057347    -0.541013   \n",
       "3          0.606086    -0.485434     0.371555    -0.571355    -0.752632   \n",
       "4         -0.610163     0.271743     0.026410    -0.070720    -0.734749   \n",
       "5          0.556017    -0.516333     0.292942    -0.510219    -0.815570   \n",
       "6         -0.730756     0.463583    -0.163860    -0.099533    -0.726087   \n",
       "7         -0.495988     0.503905     0.189778    -0.091715    -0.374547   \n",
       "8          0.485737    -0.509055     0.205669    -0.514193    -0.662836   \n",
       "9         -0.822827     0.854255    -0.094156     0.024080    -0.746256   \n",
       "10         0.308597    -0.465505     0.119238    -0.345226    -0.369610   \n",
       "11        -0.798181     0.496638    -0.034171     0.047562    -0.595119   \n",
       "12         0.271219    -0.461024     0.110460    -0.341470    -0.363755   \n",
       "13        -0.423086     0.101370    -0.082357    -0.432843    -0.241997   \n",
       "14        -0.599570     0.246430     0.119353    -0.344474    -0.688489   \n",
       "15        -0.404851     0.483681     0.266943    -0.074845    -0.191163   \n",
       "16         0.444160    -0.942218     0.674338     0.095165    -0.134390   \n",
       "17        -0.645092     0.205699    -0.168021     0.015694     0.081968   \n",
       "18        -0.311994    -0.045970    -0.014367    -0.066862    -0.617492   \n",
       "19         0.587577    -1.270145     0.994396    -0.302188    -0.737445   \n",
       "20        -0.609663     0.778754    -0.055481    -0.107367     0.102709   \n",
       "21         0.561627    -0.564382     0.305069    -0.577740    -0.671729   \n",
       "22        -0.077213     0.152694     0.163258    -0.273771    -0.668004   \n",
       "23         0.309152    -0.954982     0.490891    -0.646224    -0.320509   \n",
       "24         0.534776    -0.852070     0.461537    -0.706874    -0.598863   \n",
       "25        -0.245174    -0.237321     0.481851    -0.230093    -0.150229   \n",
       "26        -0.516591     0.204983     0.135566    -0.233460    -0.303186   \n",
       "27         0.267519    -0.735663     0.378948    -0.485111    -0.255728   \n",
       "28        -0.735772     0.705962    -0.017798    -0.401693    -0.786946   \n",
       "29         0.742424    -1.542101     1.046133    -0.560571    -0.606171   \n",
       "...             ...          ...          ...          ...          ...   \n",
       "161660     0.245717    -0.488430     0.257835    -0.369078    -0.261161   \n",
       "161661    -0.630911     0.349283    -0.416309    -0.458495    -0.735173   \n",
       "161662     0.262536    -0.122276     0.296788    -0.127964    -0.231869   \n",
       "161663     0.242595    -0.464358     0.249209    -0.368921    -0.290746   \n",
       "161664    -0.200464     0.231193    -0.065830    -0.444438    -0.510846   \n",
       "161665     0.881570     1.528145     0.740622     0.169426    -0.248190   \n",
       "161666     0.988337    -1.646624     0.716531    -0.773149    -0.871359   \n",
       "161667     0.037983    -0.094994    -0.074199    -0.034875    -0.127200   \n",
       "161668    -0.965259     0.248870     0.276991    -0.526010    -0.511087   \n",
       "161669     0.974496    -1.967171     0.844175    -0.771562    -0.724822   \n",
       "161670     0.815954    -0.484932     1.336279    -0.209755    -0.230842   \n",
       "161671     0.895123     0.092014     0.208370    -0.920188    -1.113618   \n",
       "161672    -0.742665     0.987467     0.279977    -0.102133    -0.027314   \n",
       "161673     0.811300    -0.796053     0.139067    -0.842458    -1.116935   \n",
       "161674    -0.478956     0.645151    -0.157809    -0.121785    -0.716132   \n",
       "161675     0.223869    -0.064047     0.033957     0.011888    -0.116730   \n",
       "161676    -0.612378     0.692076    -0.394390    -0.339037    -0.955438   \n",
       "161677     0.820377    -0.489251     1.338132    -0.204903    -0.230891   \n",
       "161678     0.917409    -0.551781     1.566371    -0.240333    -0.225475   \n",
       "161679     0.034666     0.216420     0.926310    -1.394728    -1.086774   \n",
       "161680    -0.305005     0.632856    -0.028393     0.042264    -0.731946   \n",
       "161681     0.265967    -0.268992     0.176062     0.111552    -0.737579   \n",
       "161682          NaN          NaN          NaN          NaN          NaN   \n",
       "161683    -0.641541     0.319282    -0.137391    -0.172887    -0.350624   \n",
       "161684     0.832171    -0.471936     1.327255    -0.222753    -0.244398   \n",
       "161685     0.191598    -0.081064    -0.083326     0.226049    -0.118010   \n",
       "161686     0.707110     0.223423     0.458621    -0.042081     0.490015   \n",
       "161687     0.988337    -1.646624     0.716531    -0.773149    -0.871359   \n",
       "161688     0.957084    -0.388220     1.394963    -0.235708    -0.378381   \n",
       "161689    -0.647476     0.530825    -0.297612    -0.350183    -0.950713   \n",
       "\n",
       "        dspt_vec195  dspt_vec196  dspt_vec197  dspt_vec198  dspt_vec199  \n",
       "0         -0.428090    -1.028613    -1.133246     0.102500     0.565922  \n",
       "1         -0.404058     0.269973    -0.481750    -0.354643     0.680272  \n",
       "2         -0.008830    -0.787262    -0.314165    -0.278627     0.008451  \n",
       "3         -0.153454     0.511548    -0.381676    -0.207083     0.606146  \n",
       "4         -0.021287    -0.570350    -0.251306    -0.455745     0.053695  \n",
       "5         -0.341179     0.393145    -0.299472    -0.290749     0.524980  \n",
       "6         -0.057904    -1.049588    -0.672365    -0.428750     0.020867  \n",
       "7          0.154913    -0.720336    -1.186944    -0.503544     0.383948  \n",
       "8         -0.327128     0.408174    -0.290132    -0.232235     0.469798  \n",
       "9          0.198009    -1.250182    -0.429074    -0.149078     0.051133  \n",
       "10        -0.179150     0.217760    -0.489515    -0.208441     0.365243  \n",
       "11        -0.197406    -0.501120    -0.153037    -0.033588     0.041597  \n",
       "12        -0.203539     0.222835    -0.473956    -0.185541     0.357674  \n",
       "13         0.144353    -0.765318    -0.809146    -0.301981     0.406393  \n",
       "14        -0.652439    -1.233067    -0.685956    -0.291937     0.518295  \n",
       "15        -0.001009    -0.788630    -1.102237    -0.454041     0.441027  \n",
       "16         0.134493    -0.558301    -0.147834    -0.064407     1.276860  \n",
       "17        -0.835978    -0.747488    -0.562503    -0.397835     0.741557  \n",
       "18        -0.068873    -1.022171    -0.477465    -0.520893     0.471650  \n",
       "19        -0.133326    -0.666286    -1.021507    -0.539557     1.246123  \n",
       "20        -0.443452    -1.046089    -0.405869    -0.566007     0.708938  \n",
       "21        -0.384483     0.165135    -0.730714    -0.392566     0.554844  \n",
       "22        -0.534815    -1.101571    -0.882956    -0.201606     0.517318  \n",
       "23        -0.003347    -0.147305    -0.241840    -0.089176     1.076155  \n",
       "24        -0.253726     0.041629    -0.600450    -0.295463     0.916539  \n",
       "25         0.017457    -1.193761    -0.993947     0.029922     0.078122  \n",
       "26        -0.477247    -1.597516    -0.973911    -0.582838     0.461862  \n",
       "27         0.017503    -0.121854    -0.206911    -0.082722     0.824879  \n",
       "28        -0.567913    -0.768496    -0.712166    -0.304170     0.336035  \n",
       "29        -0.999523    -0.809106    -0.690177    -0.823230     1.042582  \n",
       "...             ...          ...          ...          ...          ...  \n",
       "161660    -0.058940     0.227857    -0.409396    -0.266207     0.457578  \n",
       "161661    -0.514960    -1.641106    -0.524691    -0.580222     0.445142  \n",
       "161662     0.141711    -0.028734     0.171878     0.042835     0.175718  \n",
       "161663    -0.071416     0.231255    -0.378495    -0.262877     0.472049  \n",
       "161664    -0.613886    -1.462107    -0.592815    -0.477340     0.518003  \n",
       "161665     0.590384    -0.488691    -2.488954    -0.674918     0.082799  \n",
       "161666    -0.846821    -0.220222    -0.860455    -0.591583     0.792477  \n",
       "161667    -0.021638     0.035487    -0.097039    -0.065903     0.035869  \n",
       "161668    -0.310662    -0.724324    -0.916873    -0.095296     0.523926  \n",
       "161669    -1.002738    -0.539640    -0.781583    -0.713322     0.717065  \n",
       "161670     0.408734    -0.599108    -0.021398    -0.296130     0.104231  \n",
       "161671     0.247282    -2.222413    -1.531642    -0.584029     0.225495  \n",
       "161672    -0.069436    -1.131024    -0.983293     0.004859     0.125363  \n",
       "161673    -0.385801     0.329509    -0.386164    -0.186081     0.670041  \n",
       "161674    -0.209281    -0.886830    -0.303203    -0.681900     0.354667  \n",
       "161675     0.073402    -0.075450    -0.149072    -0.114950     0.115294  \n",
       "161676    -0.345813    -0.985633    -0.324630    -0.408364     0.048898  \n",
       "161677     0.449161    -0.616336    -0.045401    -0.302185     0.098645  \n",
       "161678     0.496135    -0.700243    -0.022046    -0.339213     0.111040  \n",
       "161679     0.400839    -2.184104    -1.918598    -0.335619    -0.272932  \n",
       "161680    -0.441820    -1.198673    -0.652204    -0.352233     0.440388  \n",
       "161681    -0.292620    -0.686985    -0.291699    -0.417068    -0.156539  \n",
       "161682          NaN          NaN          NaN          NaN          NaN  \n",
       "161683    -0.314643    -0.848174    -0.307982    -0.135648     0.268081  \n",
       "161684     0.431547    -0.582547    -0.031168    -0.319898     0.086200  \n",
       "161685     0.012604    -0.094667    -0.093882     0.008796     0.150246  \n",
       "161686    -0.030354    -2.341384    -0.851811    -0.119273     0.431337  \n",
       "161687    -0.846821    -0.220222    -0.860455    -0.591583     0.792477  \n",
       "161688     0.281120    -0.751800    -0.300846    -0.570374     0.675513  \n",
       "161689    -0.442733    -0.985704    -0.550679    -0.845962     0.449780  \n",
       "\n",
       "[161690 rows x 207 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# merge x and y, and drop NA\n",
    "pd_data = pd.concat([datax,datay],axis=1)\n",
    "pd_data = pd_data.dropna(axis=0, how='any') \n",
    "#Remove bad data\n",
    "pd_data = pd_data[~pd_data['discount_rate'].isin(['販売価格'])]\n",
    "pd_data = pd_data[~pd_data['discount_rate'].isin(['ダミー'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1 = pd_data[['deal_category_cd','shop_id','deal_type_cd','deal_price','discount_rate','month','duration_days']]\n",
    "data2 = pd_data.drop(['deal_category_cd','shop_id','deal_type_cd','deal_price','discount_rate','month','duration_days'], axis=1)\n",
    "data1 = normalize(data1, axis=0, norm='max')\n",
    "data2 = np.array(data2)\n",
    "data1 = DataFrame(data1)\n",
    "data2 = DataFrame(data2)\n",
    "\n",
    "pd_data_normalized = pd.concat([data1,data2],axis=1)\n",
    "\n",
    "dataset = np.array(pd_data_normalized)\n",
    "dataset = dataset.astype(np.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pick data gruop via range of y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataPick(oriData, lowLmt, upLmt, idx_y=207):\n",
    "    result = oriData[oriData[:,idx_y]<upLmt,:]\n",
    "    result = result[result[:,idx_y]>=lowLmt,:]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "556"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPick(dataset, 350, 100000).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepTrainX(dataset, percentage=0.9):\n",
    "    train = dataset[:np.trunc(len(dataset)*percentage).astype(np.int)]\n",
    "    test = dataset[np.trunc(len(dataset)*percentage).astype(np.int):]\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sepXY(dataset, feature_dim, percentage=0.9):   \n",
    "    traindata = dataset[:np.trunc(len(dataset)*percentage).astype(np.int)]\n",
    "    testdata = dataset[np.trunc(len(dataset)*percentage).astype(np.int):]\n",
    "    x_train = traindata[:,0:feature_dim]\n",
    "    y_train = traindata[:,feature_dim:]\n",
    "    x_test = testdata[:,0:feature_dim]\n",
    "    y_test = testdata[:,feature_dim:]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set class tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def biCat(amount,key):\n",
    "    result = []\n",
    "    for i in amount:\n",
    "        if i<key:\n",
    "            result = np.append(result,[0],0)\n",
    "#        elif i>0 and i<200:\n",
    "#            result = np.append(result,[1],0)\n",
    "        else:\n",
    "            result = np.append(result,[1],0)\n",
    "    result = result.reshape(-1,1)\n",
    "    result = result.astype(np.int)\n",
    "    result = np_utils.to_categorical(result, num_classes=2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Amt2Cat(amount, keys, numClasses):\n",
    "    result = []\n",
    "    for i in amount:\n",
    "        y_class = 0\n",
    "        for j in keys:\n",
    "            if i<j:\n",
    "                result = np.append(result,[y_class],0)\n",
    "                break\n",
    "            y_class = y_class+1\n",
    "            if y_class == numClasses-1:\n",
    "                result = np.append(result,[y_class],0)\n",
    "                break\n",
    "    result = np_utils.to_categorical(result.astype(np.int), num_classes=numClasses)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Traning and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  >100, 10 classes\n",
    "### Pick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_dim = 207\n",
    "\n",
    "# Set range of data\n",
    "data = dataPick(dataset, 100, 1000000)\n",
    "\n",
    "# Seperate x and y  from dataset\n",
    "x_train, y_train, x_test, y_test = sepXY(data, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bins of y\n",
    "keys=[110,120,130,140,160,190,250,350,500]\n",
    "\n",
    "# Generate tags of y\n",
    "y_train_class = Amt2Cat(y_train, keys, 10)\n",
    "y_test_class = Amt2Cat(y_test, keys, 10)\n",
    "#y_train_class = biCat(y_train, 200)\n",
    "#y_test_class = biCat(y_test, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[309, 247, 250, 206, 272, 342, 343, 340, 191, 291]\n",
      "2791\n"
     ]
    }
   ],
   "source": [
    "counter = [0,0,0,0,0,0,0,0,0,0]\n",
    "c = 0\n",
    "for i in y_train:\n",
    "    y_class = 0\n",
    "    for j in keys:\n",
    "        if i<j:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "        y_class = y_class+1\n",
    "        if y_class == 9:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "    c=c+1\n",
    "print(counter)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[31, 21, 16, 12, 37, 43, 41, 36, 25, 49]\n",
      "311\n"
     ]
    }
   ],
   "source": [
    "counter = [0,0,0,0,0,0,0,0,0,0]\n",
    "c=0\n",
    "for i in y_test:\n",
    "    y_class = 0\n",
    "    for j in keys:\n",
    "        if i<j:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "        y_class = y_class+1\n",
    "        if y_class == 9:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "    c=c+1\n",
    "print(counter)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1078"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPick(dataset, 140, 250).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grtRecall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def getPrecision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "    \n",
    "    \n",
    "def getF1(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))    \n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_dim=feature_dim, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(512, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(10, activation='softmax')) \n",
    "\n",
    "    # Define your optimizer\n",
    "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=[getF1])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -----------\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'getRecall' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-69-504d9cf62124>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Training -----------'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_class\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-c99fcb765f14>\u001b[0m in \u001b[0;36msetModel\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;31m# Define your optimizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mrmsprop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRMSprop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrho\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-08\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrmsprop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgetF1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m                 \u001b[0moutput_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m                 \u001b[0moutput_weighted_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnested_weighted_metrics\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 440\u001b[0;31m                 \u001b[0mhandle_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_metrics\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    441\u001b[0m                 \u001b[0mhandle_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_weighted_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mhandle_metrics\u001b[0;34m(metrics, weights)\u001b[0m\n\u001b[1;32m    407\u001b[0m                     metric_result = weighted_metric_fn(y_true, y_pred,\n\u001b[1;32m    408\u001b[0m                                                        \u001b[0mweights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m                                                        mask=masks[i])\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m                 \u001b[0;31m# Append to self.metrics_names, self.metric_tensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.6/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mweighted\u001b[0;34m(y_true, y_pred, weights, mask)\u001b[0m\n\u001b[1;32m    401\u001b[0m         \"\"\"\n\u001b[1;32m    402\u001b[0m         \u001b[0;31m# score_array has ndim >= 2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mscore_array\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0;31m# Cast the mask to floatX to avoid float64 upcasting in Theano\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-67-93bcb44a849c>\u001b[0m in \u001b[0;36mgetF1\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mgetF1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mprecision\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetPrecision\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mrecall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetRecall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecision\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mrecall\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'getRecall' is not defined"
     ]
    }
   ],
   "source": [
    "# training\n",
    "print('Training -----------')\n",
    "model = setModel()\n",
    "model.fit(x_train, y_train_class, epochs=6, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ------------\n",
      "311/311 [==============================] - 0s 487us/step\n",
      "test loss:  2.155497154622216\n",
      "test accuracy:  0.10662434185432851\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting ------------')\n",
    "loss, accuracy = model.evaluate(x_test, y_test_class, batch_size=20)\n",
    "\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  >100, 5 classes\n",
    "\n",
    "### Pick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 207\n",
    "\n",
    "# Set range of data\n",
    "data = dataPick(dataset, 100, 1000000)\n",
    "\n",
    "# Seperate x and y  from dataset\n",
    "x_train, y_train, x_test, y_test = sepXY(data, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bins of y\n",
    "keys=[120,150,200,350]\n",
    "\n",
    "# Generate tags of y\n",
    "y_train_class = Amt2Cat(y_train, keys, 5)\n",
    "y_test_class = Amt2Cat(y_test, keys, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPick(data, 100, 120).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_dim=feature_dim, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(512, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(5, activation='softmax')) \n",
    "\n",
    "    # Define your optimizer\n",
    "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -----------\n",
      "Epoch 1/5\n",
      "2791/2791 [==============================] - 1s 484us/step - loss: 1.5849 - acc: 0.2616\n",
      "Epoch 2/5\n",
      "2791/2791 [==============================] - 1s 347us/step - loss: 1.5445 - acc: 0.2651\n",
      "Epoch 3/5\n",
      "2791/2791 [==============================] - 1s 345us/step - loss: 1.5367 - acc: 0.2870\n",
      "Epoch 4/5\n",
      "2791/2791 [==============================] - 1s 336us/step - loss: 1.5280 - acc: 0.2977\n",
      "Epoch 5/5\n",
      "2791/2791 [==============================] - 1s 344us/step - loss: 1.5249 - acc: 0.2988\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ef18fb0b8>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "print('Training -----------')\n",
    "model = setModel()\n",
    "model.fit(x_train, y_train_class, epochs=5, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ------------\n",
      "311/311 [==============================] - 0s 509us/step\n",
      "test loss:  1.5387168812215137\n",
      "test accuracy:  0.28617363631533654\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting ------------')\n",
    "loss, accuracy = model.evaluate(x_test, y_test_class, batch_size=20)\n",
    "\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  >100, 3 classes\n",
    "\n",
    "### Pick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 207\n",
    "\n",
    "# Set range of data\n",
    "data = dataPick(dataset, 100, 1000000)\n",
    "\n",
    "# Seperate x and y  from dataset\n",
    "x_train, y_train, x_test, y_test = sepXY(data, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bins of y\n",
    "keys=[140,250]\n",
    "\n",
    "# Generate tags of y\n",
    "y_train_class = Amt2Cat(y_train, keys, 3)\n",
    "y_test_class = Amt2Cat(y_test, keys, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "608"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPick(data, 100, 120).shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_dim=feature_dim, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(512, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax')) \n",
    "\n",
    "    # Define your optimizer\n",
    "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -----------\n",
      "Epoch 1/5\n",
      "2791/2791 [==============================] - 1s 510us/step - loss: 1.0905 - acc: 0.3970\n",
      "Epoch 2/5\n",
      "2791/2791 [==============================] - 1s 347us/step - loss: 1.0527 - acc: 0.3963\n",
      "Epoch 3/5\n",
      "2791/2791 [==============================] - 1s 346us/step - loss: 1.0456 - acc: 0.4368\n",
      "Epoch 4/5\n",
      "2791/2791 [==============================] - 1s 350us/step - loss: 1.0318 - acc: 0.4274\n",
      "Epoch 5/5\n",
      "2791/2791 [==============================] - 1s 350us/step - loss: 1.0292 - acc: 0.4260\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f7ef1918c88>"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "print('Training -----------')\n",
    "model = setModel()\n",
    "model.fit(x_train, y_train_class, epochs=5, batch_size = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ------------\n",
      "311/311 [==============================] - 0s 540us/step\n",
      "test loss:  1.0484810937255908\n",
      "test accuracy:  0.398713830295483\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting ------------')\n",
    "loss, accuracy = model.evaluate(x_test, y_test_class, batch_size=20)\n",
    "\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <100, 0 or not"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_dim = 207\n",
    "\n",
    "# Set range of data\n",
    "data = dataPick(dataset, 0, 100)\n",
    "\n",
    "# Seperate x and y  from dataset\n",
    "x_train, y_train, x_test, y_test = sepXY(data, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155335, 208)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bins of y\n",
    "keys=[1]\n",
    "\n",
    "# Generate tags of y\n",
    "y_train_class = Amt2Cat(y_train, keys, 2)\n",
    "y_test_class = Amt2Cat(y_test, keys, 2)\n",
    "#y_train_class = biCat(y_train, 200)\n",
    "#y_test_class = biCat(y_test, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83150, 56651]\n",
      "139801\n"
     ]
    }
   ],
   "source": [
    "counter = [0,0]\n",
    "c = 0\n",
    "for i in y_train:\n",
    "    y_class = 0\n",
    "    for j in keys:\n",
    "        if i<j:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "        y_class = y_class+1\n",
    "        if y_class == 1:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "    c=c+1\n",
    "print(counter)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7997, 7537]\n",
      "15534\n"
     ]
    }
   ],
   "source": [
    "counter = [0,0]\n",
    "c=0\n",
    "for i in y_test:\n",
    "    y_class = 0\n",
    "    for j in keys:\n",
    "        if i<j:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "        y_class = y_class+1\n",
    "        if y_class == 1:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "    c=c+1\n",
    "print(counter)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_dim=feature_dim, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(512, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(512, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(2, activation='softmax')) \n",
    "\n",
    "    # Define your optimizer\n",
    "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -----------\n",
      "Epoch 1/5\n",
      "139801/139801 [==============================] - 15s 107us/step - loss: 0.6382 - acc: 0.6137\n",
      "Epoch 2/5\n",
      "139801/139801 [==============================] - 15s 105us/step - loss: 0.6267 - acc: 0.6333\n",
      "Epoch 3/5\n",
      "139801/139801 [==============================] - 15s 106us/step - loss: 0.6204 - acc: 0.6422\n",
      "Epoch 4/5\n",
      "139801/139801 [==============================] - 15s 106us/step - loss: 0.6183 - acc: 0.6431\n",
      "Epoch 5/5\n",
      "139801/139801 [==============================] - 15s 107us/step - loss: 0.6162 - acc: 0.6488\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7fbccb819e48>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "print('Training -----------')\n",
    "model = setModel()\n",
    "model.fit(x_train, y_train_class, epochs=5, batch_size = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ------------\n",
      "15534/15534 [==============================] - 1s 60us/step\n",
      "test loss:  0.6232094802366895\n",
      "test accuracy:  0.6693060397276842\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting ------------')\n",
    "loss, accuracy = model.evaluate(x_test, y_test_class, batch_size=20)\n",
    "\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  <100, 3 classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_dim = 207\n",
    "\n",
    "# Set range of data\n",
    "data = dataPick(dataset, 0, 100)\n",
    "\n",
    "# Seperate x and y  from dataset\n",
    "x_train, y_train, x_test, y_test = sepXY(data, feature_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(155335, 208)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set bins of y\n",
    "keys=[1,10]\n",
    "\n",
    "# Generate tags of y\n",
    "y_train_class = Amt2Cat(y_train, keys, 3)\n",
    "y_test_class = Amt2Cat(y_test, keys, 3)\n",
    "#y_train_class = biCat(y_train, 200)\n",
    "#y_test_class = biCat(y_test, 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[83150, 37244, 19407]\n",
      "139801\n"
     ]
    }
   ],
   "source": [
    "counter = [0,0,0]\n",
    "c = 0\n",
    "for i in y_train:\n",
    "    y_class = 0\n",
    "    for j in keys:\n",
    "        if i<j:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "        y_class = y_class+1\n",
    "        if y_class == 2:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "    c=c+1\n",
    "print(counter)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[7997, 5325, 2212]\n",
      "15534\n"
     ]
    }
   ],
   "source": [
    "counter = [0,0,0]\n",
    "c=0\n",
    "for i in y_test:\n",
    "    y_class = 0\n",
    "    for j in keys:\n",
    "        if i<j:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "        y_class = y_class+1\n",
    "        if y_class == 2:\n",
    "            counter[y_class] = counter[y_class]+1\n",
    "            break\n",
    "    c=c+1\n",
    "print(counter)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "37244\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for i in y_train_class:\n",
    "    if i[1] == 1:\n",
    "        counter = counter + 1\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setModel():\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Dense(256, input_dim=feature_dim, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(512, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Dense(256, activation='relu')) \n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Dense(3, activation='softmax')) \n",
    "\n",
    "    # Define your optimizer\n",
    "    rmsprop = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer=rmsprop, metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training -----------\n",
      "Epoch 1/2\n",
      "139801/139801 [==============================] - 28s 199us/step - loss: 0.8971 - acc: 0.5905\n",
      "Epoch 2/2\n",
      "139801/139801 [==============================] - 27s 190us/step - loss: 0.9308 - acc: 0.5891\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7faef5f00c18>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\n",
    "print('Training -----------')\n",
    "model = setModel()\n",
    "model.fit(x_train, y_train_class, epochs=2, batch_size = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Testing ------------\n",
      "15534/15534 [==============================] - 1s 39us/step\n",
      "test loss:  0.944731870511175\n",
      "test accuracy:  0.5189906014133117\n"
     ]
    }
   ],
   "source": [
    "print('\\nTesting ------------')\n",
    "loss, accuracy = model.evaluate(x_test, y_test_class, batch_size=20)\n",
    "\n",
    "print('test loss: ', loss)\n",
    "print('test accuracy: ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_dim = 207"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = dataset[:,0:207]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca=PCA(n_components=feature_dim)\n",
    "feature_pca=pca.fit_transform(feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(158437, 208)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pca = np.concatenate((feature_pca,dataset[:,207:208]),axis=1)\n",
    "dataset_pca.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(67290, 208)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_pca_non_zero = dataset_pca[dataset_pca[:,feature_dim]!=0,:]\n",
    "dataset_pca_non_zero.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
